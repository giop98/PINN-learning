{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lotka Volterra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is devoted to the understanding of the PINN applied to the Lotka Volterra problem. First let's formulate the problem. The equation that describe the problem are the following:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = \\alpha x - \\beta x y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dt} = \\delta x y - \\gamma y\n",
    "$$\n",
    "\n",
    "Let's suppose to fix the value of the parameters to:\n",
    "$$\n",
    "\\alpha = 0.25 \\\\\n",
    "\\beta = 0.0 \\\\\n",
    "\\gamma = 2 \\\\\n",
    "\\delta = 0.25 \\\\\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we did in the first_example notebook, let's first define the Neural_net. The strcture is really similar to the one of the first example, with the only addition of some layers, in order to make the network deeper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Neural_net(torch.nn.Module):\n",
    "    def __init__(self, n_in = 1, n_out =1):\n",
    "        super(Neural_net, self).__init__()\n",
    "\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(n_in,20)\n",
    "        self.layer2 = torch.nn.Linear(20,20)\n",
    "        self.layer3 = torch.nn.Linear(20,20)\n",
    "        self.layer_out = torch.nn.Linear(20,n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As befone, let's create a class of the PINN model, where we define the wrap_grad, used to computer the derivative of `f` with respect to `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predprey_pinn():\n",
    "    def wrap_grad(self, f,x):\n",
    "        return torch.autograd.grad(f,x,\n",
    "        grad_outputs=torch.ones_like(x),\n",
    "        retain_graph=True,\n",
    "        create_graph=True)[0]\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the normalization data. The normalization is done in a really simple way. Given the un-normalized tensor `unnormed`, the minimum of of all values is subtracted. Then, the resulting value is divided by the difference of the maximum and minimum value, that is like a range. This is used to make the value more consistent as well as the neural network training. \n",
    "\n",
    "On the other side, the `un_normalize` function is used to un-normalize the data. It does exactly the opposite process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predprey_pinn(predprey_pinn):\n",
    "    def normalize(self, id, unnormed):\n",
    "        return (unnormed - self.mins[id])/(self.maxes[id]- self.mins[id])\n",
    "\n",
    "    def un_normalize(self, id, normed):\n",
    "        return normed*(self.maxes[id] -self.mins[id])+ self.mins[id]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predprey_pinn(predprey_pinn):\n",
    "    def de_loss(self):\n",
    "        pred = self.model(self.domain)\n",
    "        x,y = (d.reshape(-1,1) for d in torch.unbind(pred, dim =1))\n",
    "        \n",
    "        dx = self.wrap_grad(x, self.domain)\n",
    "        dy = self.wrap_grad(y, self.domain)\n",
    "\n",
    "        x = self.un_normalize(0,x)\n",
    "        y = self.un_normalize(1,y)\n",
    "\n",
    "        ls0 = torch.mean((dx - (self.alpha*x -self.beta*x*y)/(self.maxes[0]-self.mins[0]) )**2)\n",
    "        ls1 = torch.mean((dy -(self.delta*x*y - y*self.gamma)/(self.maxes[1]-self.mins[1]))**2)\n",
    "        ic = torch.mean((self.c0-pred[0])**2)\n",
    "        \n",
    "        return ls0 + ls1 + ic\n",
    "    \n",
    "    def data_loss(self):\n",
    "        x,y = torch.unbind(self.model(self.t_dat), dim = 1)\n",
    "        z1 = torch.mean((x - self.x_norm)**2)\n",
    "        z2 = torch.mean((y- self.y_norm)**2)\n",
    "        return z1 + z2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
